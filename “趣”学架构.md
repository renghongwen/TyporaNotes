
算法中也包含一种哲学思想。

```
时间，空间互换 =》有舍才有得
分治算法 =》大而化小，小而化了
回溯算法 =》 大不了从头再来
动态规划 =》 开始是成功的一半
贪心算法 =》 贪得无厌常害了自己
```

计算机网络中包含的思想

```
分层理念 =》 系统分层设计。分清职责，降低耦合。
协议设计 =》 系统接口设计
重复传输 =》 分布式架构最终一致性
拥塞控制 =》 负载均衡策略
```

场景复现：

多个接口都需要走以下流程：

``` 
打印入参  =》 校验入参 =》 业务逻辑 =》 打印出参 =》 处理异常
```

这样做的弊端是  当某个步骤需要修改时，需要将所有接口中相应步骤都进行修改，会存在遗漏修改的地方

(相同的能力没有复用)

解决方案一： 抽取工具类

抽取工具类一定程度上都够实现相同能力的复用。但是，还是存在不少的弊端。

如何保证其他使用者一定能使用并正确使用这个工具类？

使用者如何确定使用哪一个工具类？

由此，我们引出了最优解决方案：

解决方案二：模版方法模式

使用模版方法模式具有以下特点：

- 确保了必要环节一定会调用
- 确保调用的顺序一定正确
- 使用方无需感知通用逻辑
- 通用逻辑方便一起升级

****



造珠子 + 串珠子 =》 责任链模式（非典型） =》流程引擎

业务流程型 流程引擎

```
jbpm ,activity ,flowable ,camunda
```

系统流程型 流程引擎 

```
liteflow 
```

典型的责任链模式中，是执行多个节点中的一个

而流程引擎则是每个节点都执行了（虽然每个节点中都有needExecute方法，这个方法实际上只是一个前置的灰度，但实际上他还是走进了这个节点）

```
流程引擎中的SOLID:
- S(单一职责原则)： 每个“珠子”职责清晰
- O(开闭原则)：业务逻辑新增则新增“珠子”
- D(依赖倒置原则)：流程引擎执行的是抽象的“珠子接口”，具体的“珠子”是使用时注入
```



拆的太猛过犹不及会导致流程编排复杂，难以进行管理。

```
那如何防止过度设计呢？
唯一的办法是：不是让系统强大，而是让系统保持灵活性
```

典型的分层方式:

- MVC：表现层 ==》 逻辑层 ==》 数据层
- MVC+前后端分离： 表现层（前端） ==》 接口层 ==》 逻辑层  ==》数据层
- 领域型设计： 表现层 （前端） ==》 接口层 ==》 服务层  ==》 领域层 ==》 数据层

分层思想虽然好，但落地实践不好搞

像领域驱动型设计中，常见的几种问题有：

- 替换三方依赖困难（DB，框架，中间件等）
- 分层边界不清导致分层“名存实亡”
- 依赖关系跨层的好与坏

解决上述问题的方案：

```
1.解耦利器之：依赖抽象而非具体
2.分层原则之边界 
	 【接口层】：对出入参仅做格式上的校验，不能设计“例如用户是否在黑名单中”这样的校验。
	 【服务层】：负责编排流程，处理RPC请求，控制同异步，不能涉及领域概念。
	 【领域层】：针对领域规则来实现具体的能力。
	 【数据层】：仅对数据做CRUD，不能涉及对数据的额外加工。
```

|      | 严格分层架构                                                 | 松散分层架构                                                 |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 利   | 1.改造影响面可控（每一层修改仅影响上层） 2.防止业务逻辑及数据外泄（每一层仅有权限访问下一层包装好的能力和数据） 3.强调持续抽象，能力内聚 | 调用方便，尤其针对简单的取数逻辑（接口层或者服务层直接向数据层取数） |
| 弊   | 简单逻辑也需要层层调用，修改复杂（每层都要改）               | 1.改造影响不可控   2.业务逻辑及数据风险不可控   3. 内聚性容易被破坏（业务逻辑可能散落在各个层上） |

**在整个分层架构中，每一层除了提供向上的能力，也提供向下的保护。**

DDD架构基础概述

```
DDD中基础层：包含数据层以及一些三方的依赖，缓存

DDD中领域层是核心,是稳定的

接口层，服务层，领域层 与 基础层 并非直接依赖，而是依赖抽象出来的接口
```

类似DDD分层架构：

![image-20240323143042099](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240323143042099.png)



```
系统分层：

为什么做系统分层？  提供复用性，降低耦合，提高可读性

如何做系统分层？ 依赖抽象而非具体， 边界规约，严格分层

什么是系统分层？ DDD，洋葱架构，六边形架构
```

**DDD(Domain Driven Design)领域驱动设计:**

![image-20240323144441770](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240323144441770.png)

![image-20240323144613744](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240323144613744.png)

DDD的特点：

- 业务概念No1（以业务领域概念为核心进行建模）
- 大家认知一致（有效对齐所有角色对业务的认知）
- 拆起来方便（高内聚，低耦合，方便拆分微服务）
- 能落地能实践（有一套行之有效的操作方法）

```
DDD设计包括 战略设计和战术设计。
战略设计主要是建立业务模型，划分业务边界，涉及的概念有领域，子域，限界上下文，领域模型。
战术设计主要是将业务模型转化为系统模型，进行代码分层，涉及的概念有实体，值对象，聚合，领域服务，领域事件。
```

 

![image-20240323151125164](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240323151125164.png)

 事件风暴：把相关的人拉起来，根据一套流程来分析业务概念，业务规则和业务过程。

```
1.梳理领域事件；事件代表某个行为的结果，是业务的重点。从事件可以正推或反推整个业务流程，并呈现其中的各种业务概念。
2.补充其他要素，如事件，命令，约束（业务规则），策略，参与方/系统
（最为重要的是尽可能让所有角色描述用户故事）
3.分析领域模型
	3.1: 从事件风暴结果中找名词
	3.2: 将这些名词根据事件风暴中的关系连接起来
4.找聚合  从领域模型中找到和其他节点直接关系最多最直接的节点
5.划分限界上下文   划分限界上下文  = 功能拆分 =  系统划分
	划分限界上下文原则 =》 业务概念无二义行性（不多包(不需要的聚合)），完整的业务子流程（不少包）
比如我们可以将商品售卖事件划分为以下几个限界上下文
商品（限界上下文）： 商品 ， 审核单
交易（限界上下文）： 订单
物流（限界上下文）： 物流单
结算（限界上下文）： 结算单，商家合约
用户（限界上下文）： 买家，卖家
金融（限界上下文）： 支付单
6. 分析领域服务  从“命令”中分析领域服务
	一个领域服务是和多个领域模型相关联的。
	领域服务是对领域模型的聚合，应用服务是对领域服务的聚合。
	应用服务和领域服务大部分情况下是一一对应的关系，更多的领域服务之间的调用会通过事件驱动的方式去实现（例如，完成商品审核动作之后，给应用服务一个事件，应用服务再出发商品上架动作），“应用层”还负责编排领域服务及外部RPC的动作。
7.划分代码层次
```

![image-20240325153215598](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240325153215598.png)

![image-20240325153426265](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240325153426265.png)

![image-20240325154427882](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240325154427882.png)

![image-20240325154624078](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240325154624078.png)

![image-20240323152809643](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240323152809643.png)

贫血模型： 只做数据的get，set。

不要盲目使用DDD，DDD中思想重于方法



****

```
前情总结：
用框架思维建系统
翻译业务建系统

搭架子  =》 串珠子  =》 系统分层  =》 DDD
     规范化      解耦            业务=》系统 

```

扩展性包括功能扩展(代码扩展性)和流量扩展(服务能力扩展性)，功能扩展是让系统有更加丰富的功能，而流量扩展则是让系统能应对更高的流量。

代码扩展性 =  设计模式 + 设计原则 + 关键意识



**设计模式使用范例:**

```
责任链模式  可以应用于参数校验上（很多参数需要校验的时候，在if中就会有很多的方法调用）
	1.定义校验接口
	2.实现校验类
	3.创建校验管理器
	4.使用处调用校验管理器中方法
责任链模式就是抽象通用类型，进行编排执行。

```

```
适配器模式 可以应用于根据不同通知类型调用不同的通知方法
	1.定义通知接口
	2.适配各种通知，即实现通知接口，例如短信通知，邮件通知等等。
	3.创建通知管理器
	4.调用处调用通知管理器中方法
适配器模式就是定义标准接口，适配外部接口
```

从上述描述中，可以看出，责任链模式和适配器模式的实现步骤都是相同的，但是从本质上来说，他们两是存在区别的，责任链模式中所有的实现都会执行，而适配器模式中则是执行适配的接口实现。 （所以关键区别在于管理器中的处理形式）

```
观察者模式
	1.定义观察者接口
	2.实现各种观察者
	3.实现主题订阅，即创建观察者管理器
	4.调用处调用观察者管理器
观察者模式就是我做完了，发出通知，关心的人自然关心（但影响不了我，业务无互相影响）。
```

观察者模式在实现步骤上也和责任链模式相同，但是不同的是责任链模式中任意一个实现出现异常，责任链就结束了，而观察者模式则不同，任意一个观察者出现异常不影响主业务的执行（即观察者模式中各个执行互不影响）。 2.观察者模式中的实现可以并行执行（可以使用线程池并发执行）。

```
策略模式 

策略模式就是定义算法族并封装，让他们可以相互替换，算法独立。核心思想是组合大于继承。
“算法族”：把行为单独抽取定义，然后封装。
```

```
装饰者模式

装饰者模式就是动态将能力扩展到对象上。
```

 上述设计模式中体现的设计原则有：

- 只做一件事
- 依赖抽象而非具体
- 对扩展开放，对修改关闭
- （仅）对变化封装
- 组合优于继承

简而言之，就是隔离，抽象，分层。

```
代码扩展性其中最为重要的是关键意识，写代码的时候，不仅要追求能用，还要追求优雅。
```

****



**流量扩展性**

挡： 恶意流量的拦截，无效流量的拦截，有效流量的降级（暂时）

扛：持续提升服务能力（AFK模型）

![image-20240327165221207](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240327165221207.png)

X轴（水平扩展）： 服务的复制，简单来说就是加机器

Y轴（垂直拆分）： 服务的分隔，即不同的服务不放在同一机器上

Z轴（单元化）： 数据与服务分区

```
水平扩展是通过加机器，复制服务的方式，提高能力
加机器不是无脑加，需要做到负载均衡和服务器设计是无状态设计，加机器才有用
```

```
负载均衡：将流量根据一些策略分配到不同的机器上
负载均衡的本质是让服务器集群整体拥有更好的抗住流量的能力，而不是某一种固定的流量分配策略。
但是基于效率，复杂读，评估和替换成本等方面的考虑，在现实情况中，我们经常还是简单粗暴的均分流量。
常见的负载均衡技术有 DNS,NGINX,LVS，eureka,nacos,zookeeper,consul
	如果LVS服务器挂掉了的话，那么负载均衡服务就不可用了，因此可以LVS和nginx可以搭配keepalived来保证负载均衡服务的高可用性

典型的互联网负载均衡设计：
1.第一道分流：DNS
	优势：域名解析必需，服务商保证高可用，智能路由，性能高
	局限：无自动发现服务能力，所有ip外网可访问，生效延迟，服务器探测
2.第二道分流：反向代理服务器 （具有稳定性风险） 可以使用keepalived和反向代理服务器虚拟ip来保证高可用
	优势：取值即用，性能高
	局限：无自动发现服务能力
3.第三道分流：注册中心
	优势：服务自动发现与分流，可以定制更强大的能力
	局限：引入额外服务，复杂度较高
```

![image-20240401150107198](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240401150107198.png)

```
"无状态设计"：每个请求都可以随机地分配到任意一个服务器上
	机器中不存储数据及缓存，将数据服务中心化。
```

```
垂直拆分 的目标是降低流量上涨过程中的服务影响，合理分配资源（机器，DB连接等）
服务拆分的好处：
	1.隔离风险，不相互影响
	2.灵活配置机器数量（机器配置）
	3.降低三方资源依赖程度
	4.区分保障等级
	5.迭代效率提升

```

```
单元化部署：一种很重要的灾备手段
数据拆分 + 服务分区
数据拆分尽可能拆细，细了可以路由，粗了不好再拆。
对于不可分割的数据，怎么进行处理呢？（比如城市的汇总数据）
 方案一： 提供通用服务，这种处理方案会导致通用服务压力很大
 方案二： 通用数据存储在通用DB中，然后数据同步到各个DB中 ，这种解决方案会存在数据一致性问题。（需要根据业务需求进行判断）
单元化部署中，每一步rpc都会重新route。单元之间是逻辑隔离，物理联通的。 

单元化部署迁移：
	1.服务迁移，数据库，DB不迁移，使用旧数据库，旧DB
	2.cache迁移，要考虑cache切流灰度还是全切。
	3.DB迁移。 
		1）单写旧
		2）双写读旧
		3）双写读新
		4）单写新
```

![image-20240401163429489](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240401163429489.png)

****



**"读"性能优化方法有：**
	缓存神器，读写分离，并发思维，异步设计，产品设计

```
缓存的多种使用形式：DB数据cache，计算数据cache，结果数据cache
缓存提效的本质是用空间换时间（及其他资源消耗）
```

|            | 优势                     | 劣势                       |
| ---------- | ------------------------ | -------------------------- |
| 本地化缓存 | 效率高（无网络开销）     | 缓存一致性保障，缓存容量大 |
| 中心化缓存 | 缓存一致性，缓存大小受限 | 网络开销，稳定性要求高     |

上述两种缓存各有场景适用，但中心化缓存普适性更强

如果两种缓存都使用的情况下，本地化缓存一般会存储一些数据量较小，不太容易变化的数据（元数据，配置类数据）

```
cache aside pattern (经典缓存更新策略) ： 写淘汰 + 读更新
能解决缓存更新不及时，缓存数据为旧值的问题 

经典缓存更新策略也不能完全解决缓存更新问题，最为重要的是在设置缓存的时候要设置过期时间。即使缓存中存在旧值，也能在一定时间后更新掉。
	宗旨：缓存中的数据一定不能“持久”。
```

```
缓存危险之“穿透”
场景再现： 使用返回数据为空的key（db中不存在的key）查询，此时所有的流量都打到db中去了，把db打挂了。
解决方案：
	1.缓存空对象，对哪些返回数据为空的key，也存储到cache中，这样流量就不会都打到db中了
	2.使用布隆过滤器。使用布隆过滤器时，会存在初始化/更新布隆过滤器问题，增加了复杂度。
		布隆过滤器的基本原理是将db中的数据初始化哈希到布隆过滤器中的bitmap中去，当一个Key过来访问时，就会使用一定的策略判断这个key是否在db中存在。
```

```
缓存危险之“雪崩”
场景再现：cache挂掉了或者集中缓存过期了，(这种情况下key在db中是存在的)
解决方案:
	1.随机过期，对不同的key设置不同的过期时间，使得不同的key不会在同一时间集中过期
	2.主动刷新，编写周期性的定时任务对缓存进行更新
	3.缓存分片，建立缓存集群，即使一个分片挂掉了，也不会导致大量的key失效打到db里去
	4.使用高可用的缓存中间件，如redis,memcached
```

```
当遇到流量大，产品多，变化多的情况下，比如典型的类电商场景（外卖等），可以使用缓存当db使用
	1.db定时同步数据到cache中，server只依赖缓存（只针对只读场景，并且cache要有高可用方案）
	2.使用NoSql数据库（但是在流量大的情况下，NoSql数据库扛不住那么大压力，因为NoSql还是基于硬盘的）
```

DB读写不分离的存在的问题：

​	连接数资源受限，大量写导致索引更新频繁，DB性能受限

对DB进行读写分离的好处：

- 理论上DB读服务可以无限扩容
- 读库可以单独建索引

存在的问题：异步数据同步，存在主备库数据不一致风险

```
通常情况下，我们不需要对主备库数据不一致做处理，大部分情况下，对数据没有那么强的一致性。如果非要做处理的话，可以在前端设置动画延迟百毫秒，再刷新数据页面。亦或是写数据时，后端强制读主数据库。
```

 并发思维：如果有一亿条数据，从中找到数据最大的前100条数据？

```
并发做法：将一亿条数据拆分成很多份10w条数据，交给不同的机器去做处理，每个机器取出当前10w数据中最大的100条数据，然后再在一个机器中去聚合出最大的100条数据。
```

 产品设计提升读请求:

- 分页查询，减少一次查询量
- 递进展示：对展示的信息区分优先级，优先请求优先级较高的数据
- 降低极致的准确性要求
- 高峰流量期间降级部分功能
- 控制（主动或被动）重试        主动重试：用户页面上进行刷新        被动重试：前端的重试机制

其他读性能优化手段:

​	优化协议，流量拦截，静态缓存，数据压缩

****



**提升写性能**

 从DB方面优化和使用方面优化两个层面进行优化。

DB方面优化

- 购买云厂商的RDS设备
- 数据分片
- 合适选型：选用合适的数据库

使用方面优化:

- 合理加锁
- 异步法
- 批量插入
- 文件法
- 缓存法

```
数据分片：（Z轴扩展）
经典的拆分方法是
	1.在机器和DB中间加一层数据服务
	2.在机器中放置jar包，作client，连接nacos做配置更新，这个jar包的作用就等同于数据服务
	3.机房隔离，机器与DB一一对应
```

|          | cache(缓存) | NoSQL（非关系型数据库） | RDBMS（关系型数据库） |
| -------- | ----------- | ----------------------- | --------------------- |
| 可靠性   | 中          | 高                      | 高                    |
| 性能     | 高          | 中高                    | 低                    |
| 易扩展   | 高          | 高                      | 低                    |
| 事务     | 低          | 低                      | 高                    |
| 复杂查询 | 低          | 低                      | 高                    |

**合理使用锁：**

```
 对于乐观锁和悲观锁的性能高低，
 我们普遍的理解是 
 	乐观锁使用了版本控制，不阻塞线程执行，性能高
 	悲观锁使用了分布式锁或DB行锁，阻塞线程，性能低
 其实，真正的差别是乐观锁将判断和更新放在一起，与DB进行一次交互，而悲观锁则是获取锁，从db中select数据，再update数据，和DB进行两次交互，而且如果使用的是分布式锁的话，获取锁也需要一定时间。
 
 虽然乐观锁性能较高，但在某些场景下，还是需要使用悲观锁的，例如：需要使用到事务（多表一致性），或者是频繁更新情况下，乐观锁失败率更高，效率更低。
 
 总的来说，用锁策略是：尽量使用乐观锁，悲观锁场景控制锁粒度。
```

**异步化**

```
常规的异步化方案：
	1.写请求 1w qps
	2.处理业务逻辑
	3.发送消息管道
	4.返回成功
	5.消息投递/拉取 1000qps 
	6.写DB
 这种方案会出现问题，比如queue消息溢出或消息丢失，消费过慢，返回给上游成功的信息，就会产生问题
 就提现场景来说，上游发送提现请求过来，我们将提现信息发送到queue中，并返回给上游成功的信息，但是提现信息在queue中丢失了，就会导致上游对账户余额进行扣减，但是银行账户没有收到提现的金额。
```

```
异步化优化方案：
	1.写请求 1w qps
	2.处理业务逻辑
	3.写缓存 （设置过期时间）
	4.发送消息管道
	5.返回客户受理 （受理中或已受理）
	6.消息投递/拉取 1000qps 
	7.写DB
	8.删除缓存
客户端接收到受理信号后：
	a.反复查询状态
	b.查询cache,查到cache中有，则说明还在写入中
	c.查到cache，查不到查db,db中存在，则处理成功，db中不存在，则说明消息溢出或者消息丢失了
```

![image-20240411142850179](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240411142850179.png)

**批量插入**

```
批量化快的原因：
	1.减少加锁释放锁耗时
	2.利用db的批量更新优化
场景再现： 一个账户，多笔入账，余额累加
请求如何hold住？
方案一：内存聚合（仅理论价值，实践价值低）
	将请求线程塞入内存queue中，在请求线程端循环检测是否已执行，然后开启一个deamon线程批量捞取内存queue中请求，然后写入db中。
方案二：流水聚合 （热点商户之缓存记账）
	将请求流水写入流水DB中，然后由 分布式调度服务 定时计算余额更新值，并更DB中的账号余额和更新流水DB中明细
	
这种方案的优势是写流水不锁库，但存在余额延迟更新问题，这个需要结合业务要求进行考虑。
```

![image-20240411144451532](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240411144451532.png)

**文件法**

```
在批量插入方案二中，如果存在写流水DB顶不住的情况下，就可以使用这个文件法了
	文件法是将信息写入HDFS（分布式文件系统）中，然后使用拆分服务对这个超大文件进行拆分，并将分片文件写入OSS/NAS中，将分片文件地址写入到queue中，由一个写入服务拉取queue中的分片信息，并在OSS/NAS中读取该分片文件，最后由写入服务将数据写入DB中
	这是一种常用的大文件写入方式，例如对账文件
```

**缓存法**

```
在对数据可靠性不需要100%的场景下，典型场景：计数类，例如：点赞数，播放数，弹幕数，在线人数等，可以使用缓存进行实现

缓存持久化方案
方案一：使用cache自带的持久化能力
	好处：省心省力，兼容各种数据结构（增删改查）
	不足：可控性差（数据筛选，频率控制） cache是全量持久化，对数据不能做到颗粒度持久化
方案二：使用分布式调度服务定时将cache中数据写入db中
	好处：控制灵活（数据筛选，持久化周期控制），对cache依赖低（容易替换，不强依赖cache）
	不足：引入依赖，实现成本高，写入服务稳定性

```

性能问题核心思路:

![image-20240412094757422](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240412094757422.png)

****

**稳定性**

故障类别有以下几种：

- 变更类：代码变更，配置变更，数据变更
- 容量类：流量突增，压测流量
- 依赖类：下游挂了，下游变更，缓存/DB容量
- 固件类：硬件故障，区域性灾害
- 安全类：流量攻击，恶意请求



系统可用性  = MTTF/ (MTTF +  MTTR)

MTTF：平均正常时间

MTTR：平均修复时间

​                                                                                        

|              |                             研发                             |                     测试                     |   上线   |                       运行                       |
| :----------: | :----------------------------------------------------------: | :------------------------------------------: | :------: | :----------------------------------------------: |
| 减少故障数量 |                   隔离：接口与接口之间隔离                   |             功能测试：测试新功能             | 发布顺序 |                                                  |
|              |            幂等：相同的请求过来会不会造成重复动作            | 回归测试：测试新代码是否会对原功能逻辑有影响 | 容器压测 |                                                  |
|              |                             限流                             |                 兼容性测试：                 | 灰度策略 |                                                  |
|              |          兼容性： 灰度上线会不会导致新旧不兼容问题           |     流量diff：将线上的case导到测试环境中     |          |                                                  |
| 提升发现速度 |                         合理记录日志                         |                                              |          |                     监控规则                     |
|              |            打点：系统使用打点的库，上报到监控系统            |                                              |          |                     合理告警                     |
|              |                                                              |                                              |          |                     日常巡检                     |
|              |                                                              |                                              |          |                     数据核对                     |
| 提升恢复速度 |                           工具建设                           |                                              |          |                       限流                       |
|              |                           容灾切换                           |                                              |          | 禁用：对故障的固件停用，不再让流量打到这台机器上 |
|              |          降级开关：在流量的情况下，降级调用下游接口          |                                              |          |                       扩容                       |
|              | 流控设置：动态调整限流配置，设置上游流量的阀值，一但超过该值，马上做限流 |                                              |          |                       回滚                       |

![image-20240412170545290](https://raw.githubusercontent.com/renghongwen/charbed/main/img/image-20240412170545290.png)

稳定性需要从**设计**抓起，需要从**幂等，隔离，降级，流控，一致性，兼容性，打日志**等方面进行考虑。



```
幂等性：同一个动作无论执行多少次，都是同样的结果。
 	例如：用户进行提现操作，提现100元，app会进行重试，网关也会进行重试，无论进行多少次重试，都要保证只体现100元。
 	
```















































